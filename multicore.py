# -*- coding: utf-8 -*-
"""Youtube Video TranScript

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZjKI9xuW3a6-iyfsHCLRcx7NKiIMsN1T
"""

from transformers import pipeline
import yt_dlp
import os
import warnings
from pydub import AudioSegment
from tqdm import tqdm
import torch
import datetime
import threading

# from multiprocessing import Pool




# Check if a GPU is available and set the device accordingly


# Set this variable to True to enable debug mode

GPU = True # Use gpu if is avilable
debug = True
multithread = True
url = 'https://youtu.be/7w412ERH7DQ'  # Replace VIDEO_ID with the actual YouTube video ID

# Disable warnings
warnings.filterwarnings("ignore", category=UserWarning, module="transformers.pipelines.base")


if not GPU:
    device = -1
    print("GPU disabled")
else:
    if torch.cuda.is_available():
        device = 0  # Use GPU device 0
        multithread = False
        print("Using GPU for ASR.")
    else:
        device = -1  # Use CPU
        print("GPU not available. Using CPU for ASR.")



# Initialize the ASR (Automatic Speech Recognition) pipeline with the Whisper model
asr_pipeline = pipeline("automatic-speech-recognition", model="openai/whisper-large-v2", device=device)

# Create a yt_dlp instance
ydl_opts = {
    'format': 'bestaudio/best',  # Specify the audio format you want
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'wav',  # Use WAV as the preferred audio codec
    }],
    'outtmpl': 'output',  # Specify the output file name without extension
}


# Download the video
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    info = ydl.extract_info(url, download=True)
    video_title = info['title']

# Replace 'your_audio_file.wav' with the path to your audio file
audio_file_path = 'output.wav'

# Define the duration of each segment in seconds (you can adjust this)
segment_duration = 10  # 5 minutes

# Create a directory to store the segments
output_directory = f'{video_title}_segments'
os.makedirs(output_directory, exist_ok=True)

# Split the audio into segments
audio = AudioSegment.from_wav(audio_file_path)
num_segments = len(audio) // (segment_duration * 1000) + 1

for i in range(num_segments):
    start_time = i * segment_duration * 1000
    end_time = min((i + 1) * segment_duration * 1000, len(audio))
    segment = audio[start_time:end_time]
    segment.export(f'{output_directory}/segment_{i + 1}.wav', format="wav")

# Get the current date and time for the filenames
current_datetime = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

# Create a debug file for writing debug output
if debug:
    debug_file = open(f"debug_data_{current_datetime}.txt", "w")

# Initialize an empty string to store the final transcription
final_transcription = ""

# Create a progress bar
progress_bar = tqdm(total=num_segments, position=0, desc="Progress", unit="segment")

final_transcription_obj = ["" for _ in range(num_segments)]

def transcribe(i):
    print(f"starting a transcribe: {i}")
    transcription = asr_pipeline(f'{output_directory}/segment_{i + 1}.wav')
    if debug:
        debug_file.write(f"Transcription for segment {i + 1}: {transcription['text']}\n")
    progress_bar.update(1)
    final_transcription_obj[i] = transcription['text']

# Perform audio-to-text transcription for each segment
if not multithread:
    for i in range(num_segments):
        r = transcribe(i)
else:
    """ print("multithread not implemented yet")
    exit() """

    threads = []
    args = [i for i in range(num_segments)]

    for arg in args:
        t = threading.Thread(target=transcribe, args=(arg,))
        threads.append(t)
        t.start()
    
    for t in threads:
        t.join()


    """ pool = Pool(processes=4)
    args = [i for i in range(num_segments)]
    results = pool.map(transcribe, args)
    pool.close()
    pool.join()
    for r in results:
        final_transcription_obj[r[1]] = r[0] """
    
    
        
progress_bar.close()

        

# Close the debug file
if debug:
    debug_file.close()

# Build the final transcript
final_transcription = ""
for i in range(num_segments):
    final_transcription += f"{final_transcription_obj[i]} "

# Create the transcript file for the final transcription
transcript_filename = f"transcript_{video_title}_{current_datetime}.txt"
with open(transcript_filename, "w") as transcript_file:
    transcript_file.write("Final Transcription:\n")
    transcript_file.write(final_transcription)

# Print the final transcription if debug is False
print("Final Transcription:")
print(final_transcription)

